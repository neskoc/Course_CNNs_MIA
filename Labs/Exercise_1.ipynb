{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to scikit-learn\n",
    "\n",
    "## In this exercise set you will familiarize yourself with the methods and datasets in scikit-learn before getting into more serious stuff\n",
    "\n",
    "Start with the breast cancer data set, availablr through scikit-learn. Really look through the data, don't jump straight to the exercises.\n",
    "\n",
    "Ideas:\n",
    "- Train several times to get different accuracies -> set random seed\n",
    "- Hotmapping to see what features to focus on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import keras\n",
    "\n",
    "# Reproducible results\n",
    "np.random.seed(42)\n",
    "data = sklearn.datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, testing\n",
    "n_training = int(data.data.shape[0] * 0.75 + 0.5) # 75% of data will be used for training\n",
    "n_val = int(data.data.shape[0] * 0.15 + 0.5)\n",
    "n_test = int(data.data.shape[0] * 0.10 + 0.5)\n",
    "print(f\"Number of samples in dataset: {data.data.shape[0]}\")\n",
    "print(f\"Split samples for training ({n_training}), validation ({n_val}), and testing ({n_test})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting of data.\n",
    "X = data.data[:n_training, :]\n",
    "y = data.target[:n_training]\n",
    "X_val = data.data[n_training:n_training + n_val, :]\n",
    "y_val = data.target[n_training:n_training + n_val]\n",
    "\n",
    "# Note! Do not touch the test data until the very end!\n",
    "X_test = data.data[n_training + n_val:, :]\n",
    "y_test = data.target[n_training + n_val:]\n",
    "\n",
    "print(f\"Training set size X  : {X.shape}\")\n",
    "print(f\"Training set size y  : {y.shape}\")\n",
    "print(f\"Validation set size X: {X_val.shape}\")\n",
    "print(f\"Validation set size y: {y_val.shape}\")\n",
    "print(f\"Test set size X      : {X_test.shape}\")\n",
    "print(f\"Test set size y      : {y_test.shape}\")\n",
    "print(f\"Output classes       : {set(y)}\")\n",
    "print(f\"Feature names        : {data.feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's the input? What's the output? What's their range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task1: Preprocess the data\n",
    "#  - Try without preprovessing, try with different kinds.\n",
    "#  - Evaluate on the validation data\n",
    "standard_scaler = sklearn.preprocessing.StandardScaler()\n",
    "standard_scaler.fit(X)\n",
    "X_ = standard_scaler.transform(X)\n",
    "X_val_ = standard_scaler.transform(X_val)\n",
    "X_test_ = standard_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit baseline model\n",
    "model_baseline = sklearn.linear_model.LogisticRegression(\n",
    "    penalty=\"none\",\n",
    "    tol=0.001,\n",
    "    fit_intercept=True,\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=10000,\n",
    "    multi_class=\"multinomial\")\n",
    "_ = model_baseline.fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model\n",
    "yhat = model_baseline.predict(X_)\n",
    "yhat_val = model_baseline.predict(X_val_)\n",
    "acc = sklearn.metrics.accuracy_score(y, yhat)\n",
    "acc_val = sklearn.metrics.accuracy_score(y_val, yhat_val)\n",
    "print(f\"Training data accuracy  : {acc}\")\n",
    "print(f\"Validation data accuracy: {acc_val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try to improve the above method somehow. Change training, testing, and validation sizes. Change maximum number of iterations, if you get a reasonable method, move on to:\n",
    "\n",
    "### Deep learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = np.shape(X_)[1]\n",
    "\n",
    "inp = Input(shape=(num_features))\n",
    "x = Dense(30)(inp)\n",
    "x = Dense(10)(x)\n",
    "x = Dense(8)(x)\n",
    "x = Dense(8)(x)\n",
    "x = Dense(4)(x)\n",
    "x = Dense(4)(x)\n",
    "outp = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inp, outp)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(lr=0.00001),\n",
    "              metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_acc', mode='max', patience=1000, restore_best_weights=True)\n",
    "loss = model.fit(X_, (y), epochs=1000, validation_data=(X_val_, (y_val)), callbacks=[es], verbose=0)\n",
    "test_loss = model.evaluate(X_test_, (y_test), verbose=0)\n",
    "epoch = np.argmin(loss.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data loss  : {loss.history['acc'][epoch]:.3f}\")\n",
    "print(f\"Validation data loss: {loss.history['val_acc'][epoch]:.3f}\")\n",
    "print()\n",
    "print(f\"Test data loss      : {test_loss[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss.history['loss'])\n",
    "plt.plot(loss.history['val_acc'])\n",
    "plt.legend(['loss', 'val_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous dataset worked fine, let's try other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducible results\n",
    "np.random.seed(42)\n",
    "data = sklearn.datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, testing\n",
    "n_training = int(data.data.shape[0] * 0.75 + 0.5) # 75% of data will be used for training\n",
    "n_val = int(data.data.shape[0] * 0.15 + 0.5)\n",
    "n_test = int(data.data.shape[0] * 0.10 + 0.5)\n",
    "print(f\"Number of samples in dataset: {data.data.shape[0]}\")\n",
    "print(f\"Split samples for training ({n_training}), validating ({n_val}), and testing ({n_test})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data[:n_training, :]\n",
    "y = data.target[:n_training]\n",
    "X_val = data.data[n_training:n_training + n_val, :]\n",
    "y_val = data.target[n_training:n_training + n_val]\n",
    "# Note! Do not touch the test data until the very end!\n",
    "X_test = data.data[n_training + n_val:, :]\n",
    "y_test = data.target[n_training + n_val:]\n",
    "\n",
    "print(f\"Training set size X  : {X.shape}\")\n",
    "print(f\"Training set size y  : {y.shape}\")\n",
    "print(f\"Validation set size X: {X_val.shape}\")\n",
    "print(f\"Validation set size y: {y_val.shape}\")\n",
    "print(f\"Test set size X      : {X_test.shape}\")\n",
    "print(f\"Test set size y      : {y_test.shape}\")\n",
    "print(f\"Output classes       : {set(y)}\")\n",
    "print(f\"Feature names        : {data.feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task1: Preprocess the data\n",
    "#  - Try without preprovessing, try with different kinds.\n",
    "#  - Evaluate on the validation data\n",
    "standard_scaler = sklearn.preprocessing.StandardScaler()\n",
    "standard_scaler.fit(X)\n",
    "X_ = standard_scaler.transform(X)\n",
    "X_val_ = standard_scaler.transform(X_val)\n",
    "X_test_ = standard_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit baseline model\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "_ = model.fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model on the test data.\n",
    "# This is only ever done once, and as the last thing we do.\n",
    "# Training another model after this, based on the performance on the test data\n",
    "# leads to biased results.\n",
    "yhat = model.predict(X_)\n",
    "yhat_val = model.predict(X_val_)\n",
    "yhat_test = model.predict(X_test_)\n",
    "acc = sklearn.metrics.mean_absolute_error(y, yhat)\n",
    "acc_val = sklearn.metrics.mean_absolute_error(y_val, yhat_val)\n",
    "acc_test = sklearn.metrics.mean_absolute_error(y_test, yhat_test)\n",
    "print(f\"Training data loss  : {acc}\")\n",
    "print(f\"Validation data loss: {acc_val}\")\n",
    "print()\n",
    "print(f\"Test data loss      : {acc_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit baseline model\n",
    "model = sklearn.linear_model.Ridge()\n",
    "_ = model.fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model on the test data.\n",
    "# This is only ever done once, and as the last thing we do.\n",
    "# Training another model after this, based on the performance on the test data\n",
    "# leads to biased results.\n",
    "yhat = model.predict(X_)\n",
    "yhat_val = model.predict(X_val_)\n",
    "yhat_test = model.predict(X_test_)\n",
    "acc = sklearn.metrics.mean_absolute_error(y, yhat)\n",
    "acc_val = sklearn.metrics.mean_absolute_error(y_val, yhat_val)\n",
    "acc_test = sklearn.metrics.mean_absolute_error(y_test, yhat_test)\n",
    "print(f\"Training data loss  : {acc}\")\n",
    "print(f\"Validation data loss: {acc_val}\")\n",
    "print(f\"Test data loss      : {acc_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = np.shape(X_)[1]\n",
    "\n",
    "inp = Input(shape=(num_features))\n",
    "x = Dense(10)(inp)\n",
    "x = Dense(10)(x)\n",
    "x = Dense(8)(x)\n",
    "x = Dense(8)(x)\n",
    "x = Dense(4)(x)\n",
    "x = Dense(4)(x)\n",
    "outp = Dense(1, activation='relu')(x)\n",
    "model = Model(inp, outp)\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['mean_absolute_error'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_mean_absolute_error', mode='min', patience=1000, restore_best_weights=True)\n",
    "loss = model.fit(X_, (y), epochs=1000, validation_data=(X_val_, (y_val)), callbacks=[es], verbose=0)\n",
    "test_loss = model.evaluate(X_test_, (y_test), verbose=0)\n",
    "epoch = np.argmin(loss.history['val_mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data loss  : {loss.history['mean_absolute_error'][epoch]}\")\n",
    "print(f\"Validation data loss: {loss.history['val_mean_absolute_error'][epoch]}\")\n",
    "print()\n",
    "print(f\"Test data loss      : {test_loss[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss.history['loss'])\n",
    "plt.plot(loss.history['val_mean_absolute_error'])\n",
    "plt.legend(['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
