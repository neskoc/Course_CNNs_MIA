{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assingment 1 - Malaria Cell Image Classification\n",
    "### Course: Convolutional Neural Networks with Applications in Medical Image Analysis\n",
    "Office hours: Mondays 13.15--15.00 (Tommy), Tuesdays 13.15--16.00 (Minh), Thursdays 08.15--12.00 (Attila)\n",
    "\n",
    "Welcome. The first assignment is based on classifying images of cells, whether they are parasitized or uninfected by malaria. It is based on an open dataset, available from Lister Hill National Center for Biomedical Communications (NIH):\n",
    "    https://lhncbc.nlm.nih.gov/publication/pub9932.\n",
    "You need to download the file ftp://lhcftp.nlm.nih.gov/Open-Access-Datasets/Malaria/cell_images.zip (as described in the assignment instructions).\n",
    "The data was preprocessed and organized for easier machine learning applications.\n",
    "\n",
    "Your task is to look through the highly customizable code below, which contains all the main steps for high accuracy classification of these data, and improve upon the model. The most important issues with the current code are noted in the comments for easier comprehension. Your tasks, to include in the report, are:\n",
    "- Reach an accuracy of at least $96~\\%$.\n",
    "- Plot the training/validating losses and accuracies. Describe where to stop training, and why that is a good choice.\n",
    "- Describe the thought process behind building your model and choosing the model hyper-parameters.\n",
    "- Describe what you think are the biggest issues with the current setup, and how to solve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages for loading the dataset\n",
    "\n",
    "import numpy as np  # Package for matrix operations, handling data\n",
    "import pandas as pd  # package for file reading\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt  # Package for plotting\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Path to dataset downloaded from the provided link\n",
    "data_path = \"cell_images\"  # Path to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out dataset\n",
    "parasitized_data = os.listdir(data_path + '/Parasitized/')\n",
    "print(parasitized_data[:2])  # the output we get are the .png files\n",
    "print(\"Number of parasitized images: \" + str(len(parasitized_data)) + '\\n')\n",
    "uninfected_data = os.listdir(data_path + '/Uninfected/')\n",
    "print(uninfected_data[:2])\n",
    "print(\"Number of non-paratisized images: \" + str(len(uninfected_data)))\n",
    "\n",
    "# NOTE: The images are in .png format, they will have to be loaded individually and handled accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some sample images\n",
    "plt.figure(figsize=(12,4))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    img = plt.imread(data_path + '/Parasitized/' + parasitized_data[i])\n",
    "    plt.imshow(img)\n",
    "    plt.title('Image size: ' + str(np.shape(img)))\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Parasitized Image Samples')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    img = plt.imread(data_path + '/Uninfected/' + uninfected_data[i + 1])\n",
    "    plt.imshow(img)\n",
    "    plt.title('Image size: ' + str(np.shape(img)))\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Uninfected Image Samples')\n",
    "plt.show()\n",
    "\n",
    "# NOTE: The images are of different size. Also they are RGB images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset preprocessing so far has been to help you, you should not change anything. However, from now on, take nothing for granted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for machine learning purposes.\n",
    "\n",
    "print(\"Loading data, this may take a while ...\")\n",
    "\n",
    "height = 16\n",
    "width = 16\n",
    "# NOTE: The size of the new images is very important.\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "for img in parasitized_data:\n",
    "    try:\n",
    "        img_read = plt.imread(data_path + '/Parasitized/' + \"/\" + img)\n",
    "        img_resize = cv2.resize(img_read, (height, width))\n",
    "        img_array = img_to_array(img_resize)\n",
    "        data.append(img_array)\n",
    "        labels.append(1)\n",
    "    except:\n",
    "        None\n",
    "\n",
    "for img in uninfected_data:\n",
    "    try:\n",
    "        img_read = plt.imread(data_path + '/Uninfected' + \"/\" + img)\n",
    "        img_resize = cv2.resize(img_read, (height, width))\n",
    "        img_array = img_to_array(img_resize)\n",
    "        data.append(img_array)\n",
    "        labels.append(0)\n",
    "    except:\n",
    "        None\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "# NOTE: The labels are 1 if the corresponding image is paratisized, 0 if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Shuffle data\n",
    "idx = np.arange(image_data.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "image_data = image_data[idx]\n",
    "labels = labels[idx]\n",
    "\n",
    "# Sizes of datasets\n",
    "print(f\"Input data shape : {np.shape(image_data)}\")\n",
    "print(f\"Output data shape: {np.shape(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(image_data, labels, test_size=0.05, random_state=101)\n",
    "\n",
    "# NOTE: The ratio of the data for training-testing is very important. The split should be reproducible.\n",
    "\n",
    "# Split testing dataset into testing and validation\n",
    "x_test, x_val = x_test[0:int(len(x_test) / 2), :], x_test[int(len(x_test) / 2):, :]\n",
    "y_test, y_val = y_test[0:int(len(y_test) / 2)], y_test[int(len(y_test) / 2):]\n",
    "\n",
    "# Two samples from the testing set for heatmapping\n",
    "x_call, y_call = x_test[[np.argmin(y_test), np.argmax(y_test)]], y_test[[np.argmin(y_test), np.argmax(y_test)]]\n",
    "\n",
    "# NOTE: Pick one parasitized and one uninfected too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.imshow(x_call[0, :, :, :])\n",
    "plt.title(\"Uninfected\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(x_call[1, :, :, :])\n",
    "plt.title(\"Infected\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the labels keras-friendly\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_val = to_categorical(y_val, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick summary of the data:\n",
    "print(f\"Training image size  : {str(x_train.shape)}\")\n",
    "print(f\"Validation image size: {str(x_val.shape)}\")\n",
    "print(f\"Testing image size   : {str(x_test.shape)}\")\n",
    "print(\"\")\n",
    "print(f\"Training label size  : {str(y_train.shape)}\")\n",
    "print(f\"Validating label size: {str(y_val.shape)}\")\n",
    "print(f\"Testing label size   : {str(y_test.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages important for building and training your model.\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, classes, channels):\n",
    "    model = Sequential()\n",
    "    inputShape = (height, width, channels)\n",
    "    chanDim = -1\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        inputShape = (channels, height, width)\n",
    "    model.add(Conv2D(8, (4, 4), strides=(2, 2), input_shape=inputShape, name='hotmap'))\n",
    "    model.add(Conv2D(8, (1, 1), strides=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(classes, activation='relu'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# NOTE: Look at the imported layers in the previous cell. Feel free to use all of them.\n",
    "# NOTE: Are you satisfied with the model being sequential? Feel free to experiment.\n",
    "# NOTE: The first activation layer is named 'hotmap' for further use for heatmapping.\n",
    "# NOTE: What activation are you using on the output layer? What range will your output have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your model.\n",
    "\n",
    "classes = 2\n",
    "channels = 3\n",
    "model = build_model(height=height, width=width, classes=classes, channels=channels)\n",
    "print(f'Size of the model input: {str(model.input.shape)}')\n",
    "model.summary()\n",
    "\n",
    "# NOTE: Are the input sizes correct?\n",
    "# NOTE: Are the output sizes correct?\n",
    "# NOTE: Is the 'hotmap' activation layer in the model?\n",
    "# NOTE: Try to imagine the model layer-by-layer and think it through. Is it doing something reasonable?\n",
    "# NOTE: Are the model parameters split \"evenly\" between the layers? Or is there one huge layer?\n",
    "# NOTE: Will the model fit into memory? Is the model too small? Is the model too large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model, try out different optimizers\n",
    "learning_rate = 0.01\n",
    "optim = optimizers.SGD(lr=learning_rate)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optim,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# NOTE: Are you satisfied with the loss function?\n",
    "# NOTE: Are you satisfied with the metric?\n",
    "# NOTE: Are you satisfied with the optimizer and its parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model onto the dataset\n",
    "batch_size = 2048\n",
    "n_epochs = 20\n",
    "\n",
    "h = model.fit(x_train, y_train, epochs=n_epochs, batch_size=batch_size)\n",
    "\n",
    "# NOTE: Plotting the accuracies and losses helps a lot.\n",
    "# NOTE: What does plotting the training data tell you? Should you plot something else?\n",
    "# NOTE: What should one do with the validation data?\n",
    "# NOTE: When should one stop? Did you overtrain? Did you train for long enough?\n",
    "# NOTE: Think about implementing Early Stopping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-GAM heatmaps\n",
    "# Note: You need to install tf-explain for this to work\n",
    "try:\n",
    "    from tf_explain.core.grad_cam import GradCAM\n",
    "\n",
    "    class_index = 0\n",
    "    explainer = GradCAM()\n",
    "    # Compute GradCAM on VGG16\n",
    "    grid = explainer.explain((x_call, y_call), model, class_index=class_index, layer_name=\"hotmap\")\n",
    "    plt.imshow(np.sum(grid, axis=2), cmap='bwr')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # NOTE: We look at the activation function of the layer called 'hotmap' from the model.\n",
    "    # NOTE: What does this image mean?\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation data. Its this value that should exceed 0.96 (i.e., 95 %).\n",
    "predictions = model.evaluate(x_val, y_val, verbose=0)\n",
    "print(f\"Validation loss    : {predictions[0]:.3f}\")\n",
    "print(f\"Validation accuracy: {predictions[1]:.3f}\")\n",
    "\n",
    "# NOTE: Is this high enough? How about varying model hyper-parameters? Perhaps implement data augmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model's test error. Note that you should _NOT_ look at this value until you are\n",
    "# completely happy with your model. Report this value in the report.\n",
    "if False:\n",
    "    predictions = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Test loss    : {predictions[0]:.3f}\")\n",
    "    print(f\"Test accuracy: {predictions[1]:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
