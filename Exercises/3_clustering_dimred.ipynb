{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_clustering_dimred.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVjJ_TfTl_Y5"
      },
      "source": [
        "# Clustering and dimensionality reduction\n",
        "\n",
        "The famous Iris dataset by Sir R. A. Fisher.\n",
        "\n",
        "More information here:\n",
        " * https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris\n",
        " * https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset\n",
        "\n",
        "We will use clustering to classify the samples, and dimensionality reduction for visualisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-ItAQvwl_pe"
      },
      "source": [
        "# Import packages\n",
        "import numpy as np\n",
        "import sklearn.datasets\n",
        "import sklearn.cluster\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make the results reproducible\n",
        "np.random.seed(42)\n",
        "\n",
        "data = sklearn.datasets.load_iris()\n",
        "\n",
        "# Split into training, validation, and test data sets\n",
        "n_train = int(data.data.shape[0] * 0.75 + 0.5)  # Train on 75 %\n",
        "n_val = int(data.data.shape[0] * 0.15 + 0.5)  # Validate on 15 %\n",
        "n_test = int(data.data.shape[0] * 0.10 + 0.5) - 1  # Test on 10 %\n",
        "\n",
        "# We won't use the targets here, even though we have them\n",
        "X = data.data[:n_train, :]\n",
        "y = data.target[:n_train]\n",
        "X_val = data.data[n_train:n_train + n_val, :]\n",
        "y_val = data.target[n_train:n_train + n_val]\n",
        "# Note! Do not use (at all!) the test data until the very end!\n",
        "X_test = data.data[n_train + n_val:, :]\n",
        "y_test = data.target[n_train + n_val:]\n",
        "\n",
        "print(f\"Training set size X  : {X.shape}\")\n",
        "print(f\"Training set size y  : {y.shape}\")\n",
        "print(f\"Validation set size X: {X_val.shape}\")\n",
        "print(f\"Validation set size y: {y_val.shape}\")\n",
        "print(f\"Test set size X      : {X_test.shape}\")\n",
        "print(f\"Test set size y      : {y_test.shape}\")\n",
        "print(f\"Output classes       : {set(y)}\")\n",
        "print(f\"Feature names        : {data.feature_names}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf-DwpMLphGC"
      },
      "source": [
        "# Task1: Preprocess the data\n",
        "#  - Try without preprocessing, try with different kinds.\n",
        "#  - Evaluate and compare models on the validation data.\n",
        "#\n",
        "# Note that we fit the preprocessing function to the training data!\n",
        "# Then we apply the learned transformation to the validation and test data sets.\n",
        "standard_scaler = sklearn.preprocessing.StandardScaler()\n",
        "standard_scaler.fit(X)\n",
        "X_ = standard_scaler.transform(X)\n",
        "X_val_ = standard_scaler.transform(X_val)\n",
        "X_test_ = standard_scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYQRinnWmS6v"
      },
      "source": [
        "# Task 2: Reduce the dimension to two, and then plot the data\n",
        "# See here for potential models to use:\n",
        "#  - https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition\n",
        "#  - https://scikit-learn.org/stable/modules/classes.html#module-sklearn.manifold\n",
        "\n",
        "dimred = \"... add your own dimensionality reduction model code here!\"\n",
        "\n",
        "dimred.fit(X_)\n",
        "T = dimred.transform(X_)\n",
        "\n",
        "print(f\"Input data shape      : {X_.shape}\")\n",
        "print(f\"Transformed data shape: {T.shape}\")\n",
        "\n",
        "plt.figure()\n",
        "for i in range(T.shape[0]):\n",
        "    if y[i] == 0:\n",
        "        h0, = plt.plot(T[i, 0], T[i, 1], '.', color=\"red\")\n",
        "    elif y[i] == 1:\n",
        "        h1, = plt.plot(T[i, 0], T[i, 1], '.', color=\"green\")\n",
        "    else:  # y[i] == 2:\n",
        "        h2, = plt.plot(T[i, 0], T[i, 1], '.', color=\"blue\")\n",
        "plt.xlabel(\"Reduced dim 1\")\n",
        "plt.xlabel(\"Reduced dim 2\")\n",
        "plt.title(\"Dimensionality reduced input data\")\n",
        "h0.set_label(\"Iris-Setosa\")\n",
        "h1.set_label(\"Iris-Versicolour\")\n",
        "h2.set_label(\"Iris-Virginica\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SKUET7Vm00R"
      },
      "source": [
        "# Fit baseline model\n",
        "model_baseline = sklearn.cluster.KMeans(\n",
        "    n_clusters=3,\n",
        "    init=\"k-means++\",\n",
        "    n_init=10,\n",
        "    max_iter=300,\n",
        "    tol=0.0001)\n",
        "_ = model_baseline.fit(X_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK-DANbVqPZF"
      },
      "source": [
        "# Plot the cluster centers with the data labels from the clustering\n",
        "\n",
        "yhat = model_baseline.predict(X_)\n",
        "\n",
        "plt.figure()\n",
        "for i in range(T.shape[0]):\n",
        "    if yhat[i] == 0:\n",
        "        plt.plot(T[i, 0], T[i, 1], '.', color=\"red\")\n",
        "    elif yhat[i] == 1:\n",
        "        plt.plot(T[i, 0], T[i, 1], '.', color=\"green\")\n",
        "    else:  # yhat[i] == 2:\n",
        "        plt.plot(T[i, 0], T[i, 1], '.', color=\"blue\")\n",
        "\n",
        "T_mu = dimred.transform(model_baseline.cluster_centers_)\n",
        "\n",
        "for i in range(T_mu.shape[0]):\n",
        "    plt.plot(T_mu[i, 0], T_mu[i, 1], 'x', markersize=20, color=\"black\")\n",
        "\n",
        "plt.xlabel(\"Reduced dim 1\")\n",
        "plt.xlabel(\"Reduced dim 2\")\n",
        "plt.title(\"Dimensionality reduced input data\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2ZINpBnpc20"
      },
      "source": [
        "# Task 3: Fit another cluster model, and plot its results.\n",
        "#  - Try to find one that performs better than K-means clustering on these data\n",
        "#  - See here for potential methods to use:\n",
        "#    https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ymrNQ-644SZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}