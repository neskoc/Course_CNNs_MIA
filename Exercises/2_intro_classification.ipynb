{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_intro_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePLSo3ldhbMO"
      },
      "source": [
        "# Classification example\n",
        "UCI Wine recognition dataset. Three-class classification.\n",
        "\n",
        "More information about the data here:\n",
        " * https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine\n",
        " * https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j97rMXHehbfe"
      },
      "source": [
        "# Import packages\n",
        "import numpy as np\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "\n",
        "# Make the results reproducible\n",
        "np.random.seed(42)\n",
        "\n",
        "data = sklearn.datasets.load_wine()\n",
        "\n",
        "# Split into training, validation, and test data sets\n",
        "n_train = int(data.data.shape[0] * 0.75 + 0.5)  # Train on 75 %\n",
        "n_val = int(data.data.shape[0] * 0.15 + 0.5)  # Validate on 15 %\n",
        "n_test = int(data.data.shape[0] * 0.10 + 0.5)  # Test on 10 %\n",
        "\n",
        "X = data.data[:n_train, :]\n",
        "y = data.target[:n_train]\n",
        "X_val = data.data[n_train:n_train + n_val, :]\n",
        "y_val = data.target[n_train:n_train + n_val]\n",
        "# Note! Do not use (at all!) the test data until the very end!\n",
        "X_test = data.data[n_train + n_val:, :]\n",
        "y_test = data.target[n_train + n_val:]\n",
        "\n",
        "print(f\"Training set size X  : {X.shape}\")\n",
        "print(f\"Training set size y  : {y.shape}\")\n",
        "print(f\"Validation set size X: {X_val.shape}\")\n",
        "print(f\"Validation set size y: {y_val.shape}\")\n",
        "print(f\"Test set size X      : {X_test.shape}\")\n",
        "print(f\"Test set size y      : {y_test.shape}\")\n",
        "print(f\"Output classes       : {set(y)}\")\n",
        "print(f\"Feature names        : {data.feature_names}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J7KKDQjiNjo"
      },
      "source": [
        "# Task1: Preprocess the data\n",
        "#  - Try without preprocessing, try with different kinds.\n",
        "#  - Evaluate and compare models on the validation data.\n",
        "#\n",
        "# Note that we fit the preprocessing function to the training data!\n",
        "# Then we apply the learned transformation to the validation and test data sets.\n",
        "standard_scaler = sklearn.preprocessing.StandardScaler()\n",
        "standard_scaler.fit(X)\n",
        "X_ = standard_scaler.transform(X)\n",
        "X_val_ = standard_scaler.transform(X_val)\n",
        "X_test_ = standard_scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5oIaLx3i3OP"
      },
      "source": [
        "# Fit baseline model\n",
        "model_baseline = sklearn.linear_model.LogisticRegression(\n",
        "    penalty=\"none\",\n",
        "    tol=0.0001,\n",
        "    fit_intercept=True,\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=100,\n",
        "    multi_class=\"multinomial\")\n",
        "_ = model_baseline.fit(X_, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC0_DAq4kHkc"
      },
      "source": [
        "# Evaluate baseline model\n",
        "yhat = model_baseline.predict(X_)\n",
        "yhat_val = model_baseline.predict(X_val_)\n",
        "acc = sklearn.metrics.accuracy_score(y, yhat)\n",
        "acc_val = sklearn.metrics.accuracy_score(y_val, yhat_val)\n",
        "print(f\"Training data accuracy  : {acc:.2f}\")\n",
        "print(f\"Validation data accuracy: {acc_val:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iqH_Kj24hDG"
      },
      "source": [
        "The model does not make any errors on the training data, and a larger error on the validation data. What does this mean? Can we do anything about it?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZJVGDWpkx2-"
      },
      "source": [
        "# Task 2: Find a better model\n",
        "#  - Try different classification methods\n",
        "#  - Evaluate them on the validation data\n",
        "#  - Beat the baseline model and select the best one you can find\n",
        "#  - See what the penalty parameter does in the LogisticRegression class:\n",
        "#    https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
        "#  - You can look here for other potential models to use here:\n",
        "#    https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model\n",
        "\n",
        "model = \"... add your own classification model code here!\"\n",
        "\n",
        "_ = model.fit(X_, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rnq0G3f7lJ8T"
      },
      "source": [
        "# Evaluate better model\n",
        "yhat = model.predict(X_)\n",
        "yhat_val = model.predict(X_val_)\n",
        "acc = sklearn.metrics.accuracy_score(y, yhat)\n",
        "acc_val = sklearn.metrics.accuracy_score(y_val, yhat_val)\n",
        "print(f\"Training data accuracy  : {acc:.2f}\")\n",
        "print(f\"Validation data accuracy: {acc_val:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOGfQmWFlVKT"
      },
      "source": [
        "# Task 3: Determine the importance of the input variables\n",
        "# ... your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuQfK309lkf4"
      },
      "source": [
        "# Evaluate the final model on the test data.\n",
        "# This is only ever done once, and as the last thing we do.\n",
        "# Training another model after this, based on the performance on the test data\n",
        "# leads to biased results.\n",
        "yhat = model.predict(X_)\n",
        "yhat_val = model.predict(X_val_)\n",
        "yhat_test = model.predict(X_test_)\n",
        "acc = sklearn.metrics.accuracy_score(y, yhat)\n",
        "acc_val = sklearn.metrics.accuracy_score(y_val, yhat_val)\n",
        "acc_test = sklearn.metrics.accuracy_score(y_test, yhat_test)\n",
        "print(f\"Training data accuracy  : {acc:.2f}\")\n",
        "print(f\"Validation data accuracy: {acc_val:.2f}\")\n",
        "print(f\"Test data accuracy      : {acc_test:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avEPMzHxlvg8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}